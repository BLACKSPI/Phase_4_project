{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExMN0Dq1J93s"
   },
   "source": [
    "# Final Project.\n",
    "We shall use the capstone guideline for this project:\n",
    "1. Business Problem.\n",
    "2. Data Understanding\n",
    "3. Data Preparation.\n",
    "4. Modelling\n",
    "5. Evaluation\n",
    "\n",
    "\n",
    "# Business Understanding\n",
    "\n",
    "## 1. Introduction:\n",
    "This modern age, social media platforms like Twitter act as  major sources for public opinions and feedback. Companies like **Apple**, understanding how customers perceive their products on Twitter is crucial for improving product development, marketing strategies, and customer relationships.\n",
    "\n",
    "So manually analyzing thousands of tweets to understand sentiment is an insane task so to speak. **Natural Language Processing (NLP)** can automate this process.\n",
    "\n",
    "The goal of this project is to develop  **NLP model** that can automatically classify the sentiment of a tweet as **positive**, **negative**, or **neutral**. This will help Apple and other stakeholders make informed decisions based on public sentiment.\n",
    "\n",
    "## 2. Stakeholders\n",
    "The stakeholders who gain from this analysis model:\n",
    "\n",
    "1.\n",
    "- **Apple and other tech companies**:\n",
    "   - **Marketing Teams**: Can use the model to track customer reactions to marketing campaigns, product launches, and other brand activities.\n",
    "   - **Product Development Teams**: Can identify which features are well-received and which need improvement based on real-time public sentiment.\n",
    "\n",
    "2.\n",
    "- **Customers**:\n",
    "   - Customers can see how their opinions are reflected in public sentiment trends, and how other users feel about Apple products.\n",
    "\n",
    "- **Competitors **:\n",
    "   - Competitors can analyze sentiment around Apple products to gain insights into Apple's strengths and weaknesses, guiding their own product strategies.\n",
    "\n",
    "## 3. Advantage of project for staeholders:\n",
    "The sentiment analysis model will provide the following benefits to stakeholders:\n",
    "- **Apple**:\n",
    "   - Helps monitor real-time public sentiment about Apple products giving faster response to customer feedback.\n",
    "- **Marketing Teams**:\n",
    "   - Allows for the measurement of campaign effectiveness and helps adjust marketing strategies based on sentiment.\n",
    "\n",
    "## 4. Implications for the Real-World Problem\n",
    "By automating sentiment analysis on Twitter, Apple and other stakeholders can gain valuable insights without the need for extensive manual work. The ability to track public sentiment in real-time means that Apple can be more responsive to customer feedback, optimize marketing efforts, and improve product offerings. This can ultimately lead to more successful product launches and higher customer satisfaction.\n",
    "\n",
    "## 5. Business Value Summary\n",
    "This NLP sentiment analysis model provides actionable insights that will help Apple make informed business decisions quickly and efficiently. By understanding public sentiment, Apple can enhance customer engagement, drive product innovation, and maintain a competitive edge in the market.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt9jf1qTKIFu"
   },
   "source": [
    "# Data Understanding\n",
    "\n",
    "## 1. Data Source\n",
    "The dataset used for this project is **CrowdFlower**, on **data.world**. The dataset has a lot of tweets that have been manually rated for sentiment by human annotators. Each tweet is labeled with one of three sentiment categories: **positive**, **negative**, or **neutral**.\n",
    "\n",
    "## 2. Dataset Size and Descriptive Statistics\n",
    "The dataset consists of over 5,000 tweets with the following key features:\n",
    "\n",
    "- **Tweet**: The textual content of the tweet (string).\n",
    "- **Sentiment**: The sentiment label assigned to each tweet (categorical: Positive, Negative, Neutral).\n",
    "  \n",
    "### Descriptive Statistics:\n",
    "Some basic statistics:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0rNAirHKQyj"
   },
   "source": [
    "\n",
    "- **The size and structure** of the dataset with basic descriptive statistics and an initial look at sentiment distribution.\n",
    "- **Limitations** of the data that may impact model performance, such as class imbalance or labeling biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hl3BE7rBLGXs"
   },
   "source": [
    "1. Loading the Dataset is essential for this project first. This allows us to use the notebook to understand the statistics of the datasource and clean it further.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "p5gAPv3pJDv4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YrXs2O-MKULt"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Apple-Twitter-Sentiment-DFE.csv', encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "Y8cUS-6SKcxT",
    "outputId": "b7512b13-c5f0-4b49-9b0b-16275581ea13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment:confidence</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "      <th>sentiment_gold</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>623495513</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6264</td>\n",
       "      <td>Mon Dec 01 19:30:03 +0000 2014</td>\n",
       "      <td>5.400000e+17</td>\n",
       "      <td>#AAPL OR @Apple</td>\n",
       "      <td>3\\nnot_relevant</td>\n",
       "      <td>#AAPL:The 10 best Steve Jobs emails ever...htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>623495514</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8129</td>\n",
       "      <td>Mon Dec 01 19:43:51 +0000 2014</td>\n",
       "      <td>5.400000e+17</td>\n",
       "      <td>#AAPL OR @Apple</td>\n",
       "      <td>3\\n1</td>\n",
       "      <td>RT @JPDesloges: Why AAPL Stock Had a Mini-Flas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>623495515</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Mon Dec 01 19:50:28 +0000 2014</td>\n",
       "      <td>5.400000e+17</td>\n",
       "      <td>#AAPL OR @Apple</td>\n",
       "      <td>3</td>\n",
       "      <td>My cat only chews @apple cords. Such an #Apple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>623495516</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5848</td>\n",
       "      <td>Mon Dec 01 20:26:34 +0000 2014</td>\n",
       "      <td>5.400000e+17</td>\n",
       "      <td>#AAPL OR @Apple</td>\n",
       "      <td>3\\n1</td>\n",
       "      <td>I agree with @jimcramer that the #IndividualIn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>623495517</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/12/14 12:14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6474</td>\n",
       "      <td>Mon Dec 01 20:29:33 +0000 2014</td>\n",
       "      <td>5.400000e+17</td>\n",
       "      <td>#AAPL OR @Apple</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nobody expects the Spanish Inquisition #AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  623495513     True      golden                  10               NaN   \n",
       "1  623495514     True      golden                  12               NaN   \n",
       "2  623495515     True      golden                  10               NaN   \n",
       "3  623495516     True      golden                  17               NaN   \n",
       "4  623495517    False   finalized                   3    12/12/14 12:14   \n",
       "\n",
       "  sentiment  sentiment:confidence                            date  \\\n",
       "0         3                0.6264  Mon Dec 01 19:30:03 +0000 2014   \n",
       "1         3                0.8129  Mon Dec 01 19:43:51 +0000 2014   \n",
       "2         3                1.0000  Mon Dec 01 19:50:28 +0000 2014   \n",
       "3         3                0.5848  Mon Dec 01 20:26:34 +0000 2014   \n",
       "4         3                0.6474  Mon Dec 01 20:29:33 +0000 2014   \n",
       "\n",
       "             id            query   sentiment_gold  \\\n",
       "0  5.400000e+17  #AAPL OR @Apple  3\\nnot_relevant   \n",
       "1  5.400000e+17  #AAPL OR @Apple             3\\n1   \n",
       "2  5.400000e+17  #AAPL OR @Apple                3   \n",
       "3  5.400000e+17  #AAPL OR @Apple             3\\n1   \n",
       "4  5.400000e+17  #AAPL OR @Apple              NaN   \n",
       "\n",
       "                                                text  \n",
       "0  #AAPL:The 10 best Steve Jobs emails ever...htt...  \n",
       "1  RT @JPDesloges: Why AAPL Stock Had a Mini-Flas...  \n",
       "2  My cat only chews @apple cords. Such an #Apple...  \n",
       "3  I agree with @jimcramer that the #IndividualIn...  \n",
       "4       Nobody expects the Spanish Inquisition #AAPL  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5SFb5wOKmSc",
    "outputId": "a7576e9f-0e88-4b2f-eb4c-ab7d4e01c88e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (3886, 12)\n",
      "Columns in dataset: Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
      "       '_last_judgment_at', 'sentiment', 'sentiment:confidence', 'date', 'id',\n",
      "       'query', 'sentiment_gold', 'text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the dataset's shape and column names\n",
    "print(f\"Shape of dataset: {df.shape}\")\n",
    "print(f\"Columns in dataset: {df.columns}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kklFFXQWLUab"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBpPeG60LWxD"
   },
   "source": [
    "Removing Unnecessary Columns\n",
    "\n",
    "Objective: Remove irrelevant columns that won't be used for analysis, making the dataset cleaner and more focused.\n",
    "\n",
    "Justification:\n",
    "\n",
    "Irrelevant columns: These columns provide metadata or details that do not contribute to the sentiment analysis model. Removing them streamlines the dataset and improves processing speed.\n",
    "\n",
    "By dropping columns such as id, query, and _unit_id, we focus only on the features that matter: the tweet text and sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bFHmkuqSK93D",
    "outputId": "47f4d517-bf11-41d3-fa7f-7ba8f4f71bcd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>#AAPL:The 10 best Steve Jobs emails ever...htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>RT @JPDesloges: Why AAPL Stock Had a Mini-Flas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>My cat only chews @apple cords. Such an #Apple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I agree with @jimcramer that the #IndividualIn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Nobody expects the Spanish Inquisition #AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0         3  #AAPL:The 10 best Steve Jobs emails ever...htt...\n",
       "1         3  RT @JPDesloges: Why AAPL Stock Had a Mini-Flas...\n",
       "2         3  My cat only chews @apple cords. Such an #Apple...\n",
       "3         3  I agree with @jimcramer that the #IndividualIn...\n",
       "4         3       Nobody expects the Spanish Inquisition #AAPL"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop columns that are not useful for analysis\n",
    "df.drop(columns=['_unit_id', '_golden', '_unit_state', '_trusted_judgments', '_last_judgment_at', 'sentiment:confidence', 'date', 'id', 'query', 'sentiment_gold'], inplace=True)\n",
    "\n",
    "# Verify the changes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TLf5wFVLjak"
   },
   "source": [
    "# Handling Missing Data\n",
    "We need to Identify and handle any missing values in the dataset. Missing data can impact model performance, so we either drop or impute it based on the context.\n",
    "\n",
    "Incase the tweet text or sentiment is missing, the data is incomplete and can't be used for analysis. Hence, we drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "id": "c-b9-IkQLZw8",
    "outputId": "ad35c6b5-d0cc-47b8-fa84-b3d68b4af3dc",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Drop rows with missing text or sentiment labels\n",
    "df.dropna(subset=['text', 'sentiment'], inplace=True)\n",
    "\n",
    "# Verify that there are no missing values\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixekb13JLotc"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSBQxEm4LrVL"
   },
   "source": [
    "## Encoding  Labels\n",
    "\n",
    "We need to Convert the sentiment labels (positive, negative, neutral) into numerical values so they can be used by machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo4KXdbRL2Y7"
   },
   "source": [
    "Label Encoding: Machine learning models can only work with numerical data. Encoding the categorical sentiment labels ensures the model can process them.\n",
    "\n",
    "This is an essential step as the analysis is a classification job. Label encoding converts categories (positive, negative, neutral) into numerical labels (0, 1, 2), making them ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "45AgzpJnLlnr",
    "outputId": "81806b76-d355-4945-9715-0d09715d9305"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2162\n",
       "0    1219\n",
       "2     423\n",
       "3      82\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the sentiment labels\n",
    "df['sentiment'] = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "# Verify the encoding\n",
    "df['sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxnPxZQ2L5cj"
   },
   "source": [
    "## Text Preprocessing\n",
    "Objective: Clean the text data to ensure it is in a format that can be processed by the machine learning model. This includes removing special characters, stop words, and unnecessary spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWltmhrYLxtE",
    "outputId": "38d117b8-322a-4de7-da0c-07c6057ae2c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the stopwords resource\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3sbslCPMU-L",
    "outputId": "3111d91e-f48b-4dfb-ea45-6a1bcd7e24d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OKOEi4ZHL8mU",
    "outputId": "3eec39a5-8944-4992-813f-83179518dccd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK resources if you haven't already\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Return cleaned text as a string\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZk7ZckuNJqT"
   },
   "source": [
    "### Feature Engineering\n",
    "\n",
    "We Convert the cleaned text into a numerical format that machine learning models can understand. We'll use TF-IDF for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Hhg2rHVMn7F",
    "outputId": "842d2b76-c747-48e7-9c38-ca63b79209ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3886, 5000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features to avoid overfitting\n",
    "\n",
    "# Fit and transform the 'text' column\n",
    "X = vectorizer.fit_transform(df['text']).toarray()\n",
    "\n",
    "# Verify the shape of the feature matrix\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W1is4i02OBHr",
    "outputId": "32f90a76-c2b2-4d04-a886-9fa0f680c510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentiment', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the column names of your dataframe\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BhcsZOdOOluU",
    "outputId": "d4c1bf92-03c5-414c-de64-3fc308956de5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4eBDxOtOtTk",
    "outputId": "0b54ceb6-9c65-4818-c47f-4c3535782b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\User/nltk_data', 'C:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\learn-env\\\\nltk_data', 'C:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\learn-env\\\\share\\\\nltk_data', 'C:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\learn-env\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DxtUM8dZO15T"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('/root/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4naZGq4O3D8",
    "outputId": "acc035b0-af0e-4d0a-ba80-c78986115068"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXQtzM35O5NM",
    "outputId": "d7eef131-af1e-4c47-f27a-0c8af78185fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U2Y0H2dVPGtJ",
    "outputId": "339e01f4-9091-46b2-f7a3-6e603f52f47e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "He_DA4tlPKN9"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zU1ADVDYPNPE",
    "outputId": "1f9de7e5-9fc8-4f9c-e1d9-d0136714d6ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punkt tokenizer is available!\n"
     ]
    }
   ],
   "source": [
    "# Check if 'punkt' is available\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "try:\n",
    "    word_tokenize(\"This is a test sentence.\")\n",
    "    print(\"Punkt tokenizer is available!\")\n",
    "except LookupError as e:\n",
    "    print(\"Error: \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HkabNohdOHuj",
    "outputId": "c46b1933-2c8b-4562-865b-75b4c57273bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  #AAPL:The 10 best Steve Jobs emails ever...htt...   \n",
      "1  RT @JPDesloges: Why AAPL Stock Had a Mini-Flas...   \n",
      "2  My cat only chews @apple cords. Such an #Apple...   \n",
      "3  I agree with @jimcramer that the #IndividualIn...   \n",
      "4       Nobody expects the Spanish Inquisition #AAPL   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0                aaplthe best steve jobs emails ever  \n",
      "1  rt jpdesloges aapl stock miniflash crash today...  \n",
      "2                    cat chews apple cords applesnob  \n",
      "3  agree jimcramer individualinvestor trade apple...  \n",
      "4            nobody expects spanish inquisition aapl  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK resources if you haven't already\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Return cleaned text as a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the cleaning function to the 'text' column to create 'cleaned_text'\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Verify the first few rows to ensure 'cleaned_text' is created\n",
    "print(df[['text', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_WS8LO1NlqL"
   },
   "source": [
    "Visualizations part of EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58ASEderNp_L"
   },
   "source": [
    "## Class Distribution (Sentiment Distribution)\n",
    "Bar Plot to show the distribution of sentiments in your dataset (e.g., how many positive, negative, and neutral sentiments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "Y3pPPihTNLy0",
    "outputId": "c73ea491-aab5-4756-9796-58dc663b6983",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVyklEQVR4nO3dfbRddX3n8ffH8GgBhUVkYhIJjhlroDUtEUVqRbEFHBnQBTasVsDBiVp0fKodEKvSaUY7neLTFBhaWQTbAWmVBXT5RBkQH3gwsEAIEUzlKRIhwCDBUjTxO3+cneF4ucnv5nLPPffmvl9r7XX2+e29f/t79kru5+7f3mffVBWSJG3Ns4ZdgCRp6jMsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhou5PknCR/Muw6xmsi60/ygiSPJ5nVvb86ydsmou+uv68kOXGi+tPUZVhoUiT5rSTfSfKTJI8k+XaSl01Avycl+VZ/W1W9o6r+6zPtexy1fCzJ3zbWuTvJE0k2JHm0OybvSPL//y+Otf6ur9dtbZ2qureqdquqTWP/JFvc39M+X1UdWVUrnmnfmvoMCw1ckj2AfwQ+C+wFzAXOAJ4cZl1DdFRV7Q7sC3wC+C/A5yZ6J0l2mOg+NYNVlZPTQCdgCfBoY53/CKwG/i/wNWDfvmUFvAP4Qbf8r4AALwH+FdgEPL55H8D5wJ9184cCa4E/Bh4E1gHHAK8H7gQeAT7Ut69nAacC/ww8DFwM7NUtW9DVciJwL/AQcHq37AjgZ8DPu1pu2cLnvBt43Yi2g4BfAAeMUv/e9IL20a7Wb3Y1fr7b5oluf3/cV9/JXX3X9LXt0PV3NfBx4AbgJ8ClfZ/vUGDtaPVu6fN1/b2t79h9GLinO9YXAM9pHTun6TF5ZqHJcCewKcmKJEcm2bN/YZJjgA8BbwJm0/uBeOGIPt4AvAx4KfBm4PCqWk0vRK6t3lDLc7ew/38D7ELvjOYjwF8DfwAcCLwK+EiSF3br/md6YfJq4Pk8FU79fgt4MXBYt+1LquqrwH8DvtDV8tIxHBcAquoGeoH2qlEWf6BbNhvYh95xqqp6C70fukd1+/vvfdu8ml6QHr6FXZ5AL5yfD2wEPjOGGsfy+U7qptcALwR2A/7niHWeduxa+9bUYFho4KrqMXo/JIreD+r1SS5Lsk+3ytuBj1fV6qraSO+H0uIk+/Z184mqerSq7gWuAhZvQwk/B5ZX1c+Bi+j9tv7pqtpQVauAVcCv99VyelWtraongY8Bx44Y0jmjqp6oqluAW+gF2DN1P70hutFqn0PvTOvnVfXNqmo90O1jVfXTqnpiC8s/X1W3VdVPgT8B3rz5Avgz9PvAmVX1w6p6HDgNWDoJx06TwLDQpOiC4KSqmgccQO+32k91i/cFPt1d8H2U3nBL6J0JbPbjvvl/ofdb61g9XE9d4N38A/SBvuVP9PW3L3BJXy2r6Q1z7dO3/jOpZUvm0vvcI/0FsAb4epIfJjl1DH3dtw3L7wF2pBegz9Tzu/76+96BwR87TQLDQpOuqr5Pb1z+gK7pPuDtVfXcvmnXqvrOWLqb4PLuA44cUcsuVfWjQdXS3RU2F/jWyGXd2c8HquqFwFHA+5Mc1thfq475ffMvoHf28hDwU+DZfXXNojf8NdZ+76cXtv19b+SXg1nTlGGhgUvyq0k+kGRe934+cDxwXbfKOcBpSfbvlj8nyXFj7P4BYF6SnSao3HOA5ZuHwJLMTnL0NtSyoP822K1JskeSN9AbGvvbqrp1lHXekORFSQI8Ru8sZ/NZ0gP0rg1sqz9IsijJs4E/Bf6hO/O6E9glyb9PsiO9i9U7b8PnuxB4X5L9kuzGU9c4No6jRk0xhoUmwwbg5cD1SX5KLyRuo3fxlqq6BPhz4KIkj3XLjhxj3/+H3jWHHyd5aAJq/TRwGb1hnw1drS8f47Z/370+nOSmrax3edf3fcDpwJnAW7ew7kLgn+jdgXQtcFZVXd0t+zjw4W7I7I/GWCP07qQ6n96Q0C70LupTVT8B/hD4G+BH9M401m7D5zuv6/sa4C56d6q9exvq0hSW9rUySdJM55mFJKnJsJAkNRkWkqQmw0KS1LTdPmhs7733rgULFgy7DEmaVm688caHqmr2yPbtNiwWLFjAypUrh12GJE0rSe4Zrd1hKElSk2EhSWoyLCRJTYaFJKnJsJAkNRkWkqQmw0KS1GRYSJKaDAtJUtN2+w1uTa57//TXhl3ClPGCjzztD95J055nFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoGFhZJ5ie5KsnqJKuSvKdr3yvJFUl+0L3u2bfNaUnWJLkjyeF97QcmubVb9pkkGVTdkqSnG+SZxUbgA1X1EuAVwClJFgGnAldW1ULgyu493bKlwP7AEcBZSWZ1fZ0NLAMWdtMRA6xbkjTCwMKiqtZV1U3d/AZgNTAXOBpY0a22Ajimmz8auKiqnqyqu4A1wEFJ5gB7VNW1VVXABX3bSJImwaRcs0iyAPgN4Hpgn6paB71AAZ7XrTYXuK9vs7Vd29xufmT7aPtZlmRlkpXr16+f0M8gSTPZwMMiyW7AF4H3VtVjW1t1lLbaSvvTG6vOraolVbVk9uzZ216sJGlUAw2LJDvSC4q/q6ovdc0PdENLdK8Pdu1rgfl9m88D7u/a543SLkmaJIO8GyrA54DVVXVm36LLgBO7+ROBS/valybZOcl+9C5k39ANVW1I8oquzxP6tpEkTYJB/lnVQ4C3ALcmublr+xDwCeDiJCcD9wLHAVTVqiQXA7fTu5PqlKra1G33TuB8YFfgK90kSZokAwuLqvoWo19vADhsC9ssB5aP0r4SOGDiqpMkbQu/wS1JajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoMC0lSk2EhSWoyLCRJTYaFJKnJsJAkNRkWkqQmw0KS1GRYSJKaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoMC0lSk2EhSWoyLCRJTYaFJKnJsJAkNRkWkqQmw0KS1GRYSJKaBhYWSc5L8mCS2/raPpbkR0lu7qbX9y07LcmaJHckObyv/cAkt3bLPpMkg6pZkjS6QZ5ZnA8cMUr7J6tqcTd9GSDJImApsH+3zVlJZnXrnw0sAxZ202h9SpIGaGBhUVXXAI+McfWjgYuq6smqugtYAxyUZA6wR1VdW1UFXAAcM5CCJUlbNIxrFu9K8r1umGrPrm0ucF/fOmu7trnd/Mj2USVZlmRlkpXr16+f6Lolacaa7LA4G/i3wGJgHfCXXfto1yFqK+2jqqpzq2pJVS2ZPXv2MyxVkrTZpIZFVT1QVZuq6hfAXwMHdYvWAvP7Vp0H3N+1zxulXZI0iSY1LLprEJu9Edh8p9RlwNIkOyfZj96F7Buqah2wIckrurugTgAuncyaJUmww6A6TnIhcCiwd5K1wEeBQ5MspjeUdDfwdoCqWpXkYuB2YCNwSlVt6rp6J707q3YFvtJNkqRJNLCwqKrjR2n+3FbWXw4sH6V9JXDABJYmSdpGfoNbktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoMC0lSk2EhSWoyLCRJTYaFJKnJsJAkNRkWkqQmw0KS1DSmsEhyyFjaJEnbp7GeWXx2jG2SpO3QVv8Gd5KDgVcCs5O8v2/RHsCsQRYmSZo6thoWwE7Abt16u/e1PwYcO6iiJElTy1bDoqq+AXwjyflVdc8k1TQpDvzgBcMuYcq48S9OGHYJkqa41pnFZjsnORdY0L9NVb12EEVJkqaWsYbF3wPnAH8DbBpcOZKkqWisYbGxqs4eaCWSpClrrLfOXp7kD5PMSbLX5mmglUmSpoyxnlmc2L1+sK+tgBdObDmSpKloTGFRVfsNuhBJ0tQ1prBIMuq9lVXl/aeSNAOMdRjqZX3zuwCHATcBhoUkzQBjHYZ6d//7JM8BPj+QiiRJU854H1H+L8DCiSxEkjR1jfWaxeX07n6C3gMEXwJcPKiiJElTy1ivWfyPvvmNwD1VtXYA9UiSpqAxDUN1DxT8Pr0nz+4J/GyQRUmSppax/qW8NwM3AMcBbwauT+IjyiVphhjrMNTpwMuq6kGAJLOBfwL+YVCFSZKmjrHeDfWszUHReXgbtpUkTXNjPbP4apKvARd2738P+PJgSpIkTTVbPTtI8qIkh1TVB4H/Bfw68FLgWuDcxrbnJXkwyW19bXsluSLJD7rXPfuWnZZkTZI7khze135gklu7ZZ9JknF+VknSOLWGkj4FbACoqi9V1fur6n30zio+1dj2fOCIEW2nAldW1ULgyu49SRYBS4H9u23OSjKr2+ZsYBm9LwEuHKVPSdKAtcJiQVV9b2RjVa2k9ydWt6iqrgEeGdF8NLCim18BHNPXflFVPVlVdwFrgIOSzAH2qKprq6roPYvqGCRJk6oVFrtsZdmu49jfPlW1DqB7fV7XPhe4r2+9tV3b3G5+ZPuokixLsjLJyvXr14+jPEnSaFph8d0k/2lkY5KTgRsnsI7RrkPUVtpHVVXnVtWSqloye/bsCStOkma61t1Q7wUuSfL7PBUOS4CdgDeOY38PJJlTVeu6IabNt+OuBeb3rTcPuL9rnzdKuyRpEm31zKKqHqiqVwJnAHd30xlVdXBV/Xgc+7uMp/5E64nApX3tS5PsnGQ/eheyb+iGqjYkeUV3F9QJfdtIkibJWP+exVXAVdvScZILgUOBvZOsBT4KfAK4uBvGupfe40OoqlVJLgZup/egwlOqalPX1Tvp3Vm1K/CVbpIkTaKxfilvm1XV8VtYdNgW1l8OLB+lfSVwwASWJknaRj6yQ5LUZFhIkpoMC0lSk2EhSWoyLCRJTYaFJKnJsJAkNRkWkqQmw0KS1GRYSJKaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoMC0lSk2EhSWoyLCRJTYaFJKnJsJAkNRkWkqQmw0KS1GRYSJKaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUNJSyS3J3k1iQ3J1nZte2V5IokP+he9+xb/7Qka5LckeTwYdQsSTPZMM8sXlNVi6tqSff+VODKqloIXNm9J8kiYCmwP3AEcFaSWcMoWJJmqqk0DHU0sKKbXwEc09d+UVU9WVV3AWuAgya/PEmauYYVFgV8PcmNSZZ1bftU1TqA7vV5Xftc4L6+bdd2bU+TZFmSlUlWrl+/fkClS9LMs8OQ9ntIVd2f5HnAFUm+v5V1M0pbjbZiVZ0LnAuwZMmSUdeRJG27oZxZVNX93euDwCX0hpUeSDIHoHt9sFt9LTC/b/N5wP2TV60kadLDIsmvJNl98zzwu8BtwGXAid1qJwKXdvOXAUuT7JxkP2AhcMPkVi1JM9swhqH2AS5Jsnn//7uqvprku8DFSU4G7gWOA6iqVUkuBm4HNgKnVNWmIdQtSTPWpIdFVf0QeOko7Q8Dh21hm+XA8gGXJknagql066wkaYoa1t1QkrbgkM8eMuwSpoxvv/vbwy5BHc8sJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoMC0lSk2EhSWoyLCRJTYaFJKnJsJAkNRkWkqQmw0KS1GRYSJKaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoMC0lSk2EhSWraYdgFSNIgfeO3Xz3sEqaMV1/zjXFv65mFJKnJsJAkNRkWkqQmw0KS1DRtwiLJEUnuSLImyanDrkeSZpJpERZJZgF/BRwJLAKOT7JouFVJ0swxLcICOAhYU1U/rKqfARcBRw+5JkmaMVJVw66hKcmxwBFV9bbu/VuAl1fVu0astwxY1r19MXDHpBY6PnsDDw27iO2Ex3JieTwn1nQ5nvtW1eyRjdPlS3kZpe1pKVdV5wLnDr6ciZNkZVUtGXYd2wOP5cTyeE6s6X48p8sw1Fpgft/7ecD9Q6pFkmac6RIW3wUWJtkvyU7AUuCyIdckSTPGtBiGqqqNSd4FfA2YBZxXVauGXNZEmVbDZlOcx3JieTwn1rQ+ntPiArckabimyzCUJGmIDAtJUpNhMSQ+vmTiJDkvyYNJbht2LduDJPOTXJVkdZJVSd4z7JqmqyS7JLkhyS3dsTxj2DWNl9cshqB7fMmdwO/Quy34u8DxVXX7UAubppL8NvA4cEFVHTDseqa7JHOAOVV1U5LdgRuBY/z3ue2SBPiVqno8yY7At4D3VNV1Qy5tm3lmMRw+vmQCVdU1wCPDrmN7UVXrquqmbn4DsBqYO9yqpqfqebx7u2M3Tcvf0A2L4ZgL3Nf3fi3+Z9QUlGQB8BvA9UMuZdpKMivJzcCDwBVVNS2PpWExHGN6fIk0TEl2A74IvLeqHht2PdNVVW2qqsX0njxxUJJpOVRqWAyHjy/RlNaNr38R+Luq+tKw69keVNWjwNXAEcOtZHwMi+Hw8SWasrqLsp8DVlfVmcOuZzpLMjvJc7v5XYHXAd8falHjZFgMQVVtBDY/vmQ1cPF29PiSSZfkQuBa4MVJ1iY5edg1TXOHAG8BXpvk5m56/bCLmqbmAFcl+R69XxKvqKp/HHJN4+Kts5KkJs8sJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIIyQ5vXtC6Pe620ZfPo4+FvffbprkPwz66cJJDk3yykHuQzPXtPizqtJkSXIw8AbgN6vqySR7AzuNo6vFwBLgywBVdRmD/+LlofSevvudAe9HM5Dfs5D6JHkT8NaqOmpE+4HAmcBuwEPASVW1LsnV9B6y9xrgucDJ3fs1wK7Aj4CPd/NLqupdSc4HngB+FdgXeCtwInAwcH1VndTt83eBM4CdgX/u6no8yd3ACuAoek8xPQ74V+A6YBOwHnh3VX1zQg+OZjSHoaRf9nVgfpI7k5yV5NXdc5I+CxxbVQcC5wHL+7bZoaoOAt4LfLR77PxHgC9U1eKq+sIo+9kTeC3wPuBy4JPA/sCvdUNYewMfBl5XVb8JrATe37f9Q1372cAfVdXdwDnAJ7t9GhSaUA5DSX2639wPBF5F72zhC8CfAQcAV/Qem8QsYF3fZpsftHcjsGCMu7q8qirJrcADVXUrQJJVXR/zgEXAt7t97kTvkSaj7fNNY/+E0vgYFtIIVbWJ3tNBr+5+mJ8CrKqqg7ewyZPd6ybG/n9q8za/6Jvf/H6Hrq8rqur4CdynNG4OQ0l9krw4ycK+psX0HvY4u7v4TZIdk+zf6GoDsPszKOU64JAkL+r2+ewk/27A+5S2yLCQftluwIokt3dPCl1E7/rDscCfJ7kFuBlo3aJ6FbCou/X297a1iKpaD5wEXNjVcR29C+Jbcznwxm6fr9rWfUpb491QkqQmzywkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmgwLSVLT/wPmCZjmrpVl2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='sentiment', data=df)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\learn-env\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\envs\\learn-env\\lib\\site-packages\\seaborn\\categorical.py:250: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  vals = pd.Series(vals, index=index)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-f8f4042311b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mx_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtop25\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'h'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Top 20 Stop Words Used'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\seaborn\\_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m             )\n\u001b[0;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mbarplot\u001b[1;34m(x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge, ax, **kwargs)\u001b[0m\n\u001b[0;32m   3167\u001b[0m ):\n\u001b[0;32m   3168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3169\u001b[1;33m     plotter = _BarPlotter(x, y, hue, data, order, hue_order,\n\u001b[0m\u001b[0;32m   3170\u001b[0m                           \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3171\u001b[0m                           \u001b[0morient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[0;32m   1584\u001b[0m         self.establish_variables(x, y, hue, data, orient,\n\u001b[0;32m   1585\u001b[0m                                  order, hue_order, units)\n\u001b[1;32m-> 1586\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1587\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimate_statistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mestablish_colors\u001b[1;34m(self, color, palette, saturation)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[1;31m# Determine the gray color to use for the lines framing the plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[0mlight_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolorsys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrgb_to_hls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrgb_colors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mlum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlight_vals\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m.6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrgb2hex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].astype(str)\n",
    "\n",
    "corpus=[]\n",
    "new= df['cleaned_text'].str.split()\n",
    "new=new.values.tolist()\n",
    "corpus=[word for i in new for word in i]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "dic=defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop_words:\n",
    "        dic[word]+=1\n",
    "        \n",
    "top25 = sorted(dic.items(), key=lambda item: item[1], reverse=True)[:20]\n",
    "\n",
    "y_val = [x[0] for x in top25]\n",
    "x_val = [x[1] for x in top25]\n",
    "\n",
    "ax = sns.barplot(x_val,y_val, orient='h')\n",
    "ax = ax.set_title('Top 20 Stop Words Used')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('images/stopwords.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9p1ke4dNwAT"
   },
   "source": [
    "#Word Cloud for Most Frequent Terms\n",
    "Visualize the most common words in your dataset, which can give you an idea of the frequent terms in the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wsP0FFLRPgok",
    "outputId": "31ece707-29d5-4d60-886f-b776ad1120d4"
   },
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "zxK9F1r6Nt0U",
    "outputId": "304b39eb-177a-4bfa-f81a-65779825d0dc"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Combine all cleaned text into a single string\n",
    "all_text = ' '.join(df['cleaned_text'])\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.title('Most Frequent Words in the Tweets')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyigpVgXPpZc"
   },
   "source": [
    "## Top Positive and Negative Words\n",
    "You can visualize the most frequent words in positive and negative sentiment categories. Here's a quick way to extract and plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0edGuD5TNzyc",
    "outputId": "8cce4414-e1da-4ec2-b0a1-9edbaa32bb96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    2162\n",
      "0    1219\n",
      "2     423\n",
      "3      82\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "uQ4i7hGpPsjl"
   },
   "outputs": [],
   "source": [
    "#filter for positive and negative tweets\n",
    "positive_tweets = df[df['sentiment'] == 1]\n",
    "negative_tweets = df[df['sentiment'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "nZX7VrUDP9Kz"
   },
   "outputs": [],
   "source": [
    "#Ectract text for generation\n",
    "positive_text = ' '.join(positive_tweets['cleaned_text'])\n",
    "negative_text = ' '.join(negative_tweets['cleaned_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "jYV-4YxEQC0E",
    "outputId": "10331e1d-8608-41c1-e8b1-0d3be2066841"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only supported for TrueType fonts",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-983e52c73b75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Generate word clouds for both positive and negative sentiments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpositive_wordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'white'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mnegative_wordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'white'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnegative_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \"\"\"\n\u001b[1;32m--> 642\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    622\u001b[0m         \"\"\"\n\u001b[0;32m    623\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    451\u001b[0m                 \u001b[0mfont_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m                 self.generate_from_frequencies(dict(frequencies[:2]),\n\u001b[0m\u001b[0;32m    454\u001b[0m                                                max_font_size=self.height)\n\u001b[0;32m    455\u001b[0m                 \u001b[1;31m# find font sizes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    509\u001b[0m                     font, orientation=orientation)\n\u001b[0;32m    510\u001b[0m                 \u001b[1;31m# get size of resulting text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m                 \u001b[0mbox_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextbbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransposed_font\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"lt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m                 \u001b[1;31m# find possible places using integral image:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m                 result = occupancy.sample_position(box_size[3] + self.margin,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\PIL\\ImageDraw.py\u001b[0m in \u001b[0;36mtextbbox\u001b[1;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[0mfont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetfont\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreeTypeFont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Only supported for TrueType fonts\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"RGBA\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0membedded_color\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfontmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m         bbox = font.getbbox(\n",
      "\u001b[1;31mValueError\u001b[0m: Only supported for TrueType fonts"
     ]
    }
   ],
   "source": [
    "#Generate worldcloud plot\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate word clouds for both positive and negative sentiments\n",
    "positive_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_text)\n",
    "negative_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(negative_text)\n",
    "\n",
    "# Plot the word clouds\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "plt.title(\"Positive Sentiment\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
    "plt.title(\"Negative Sentiment\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Iug9z4UQVki"
   },
   "source": [
    "## **Modeling**\n",
    "In this section, we will build multiple models iteratively, starting from a simple baseline model and progressively adding more complexity. We will compare the performance of each model and justify the improvements over the previous ones based on the results. The goal is to demonstrate an effective approach to model-building, starting from a basic model and moving toward more advanced techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vR8PrFNFQo7c",
    "outputId": "6ce19297-3f39-4f2f-e945-87fd1dbee105"
   },
   "outputs": [],
   "source": [
    "#Train and test splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'X' is the feature matrix (TF-IDF features) and 'y' is the target variable (sentiment)\n",
    "X = vectorizer.transform(df['cleaned_text']).toarray()  # Or you can use the vectorized version from earlier\n",
    "y = df['sentiment']  # Target variable\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the training and test sets\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5c-zh6DQcUU"
   },
   "source": [
    "## 1. Baseline Model: Logistic Regression\n",
    "Model Description:\n",
    "We begin by training a Logistic Regression model as our baseline. Logistic Regression is a simple yet effective model for binary classification tasks. It will provide us with an initial benchmark to compare more complex models against.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5jTmVI_QHMc",
    "outputId": "6e86e126-072c-46e1-de7e-9e32ef6b94fc"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize and train the baseline Logistic Regression model\n",
    "baseline_model = LogisticRegression(max_iter=1000)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "baseline_y_pred = baseline_model.predict(X_test)\n",
    "\n",
    "# Evaluate the baseline model\n",
    "baseline_accuracy = accuracy_score(y_test, baseline_y_pred)\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, baseline_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PedTdC8QQ3Hd"
   },
   "source": [
    "## 2. Improved Model: Random Forest Classifier\n",
    "Model Description:\n",
    "To improve upon the baseline Logistic Regression model, we introduce a Random Forest Classifier. Random Forest is an ensemble method that builds multiple decision trees and combines their results to improve prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHCPc1TjQiqU",
    "outputId": "57a83537-c161-4e27-9917-f439c4f852fa"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, rf_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MY3WVgUgQ_AT"
   },
   "source": [
    "## 3. Further Improvement: Support Vector Machine (SVM)\n",
    "Model Description:\n",
    "Next, we experiment with a Support Vector Machine (SVM). The SVM is a powerful classifier, especially effective in high-dimensional spaces, such as the feature space generated by text data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjbY1ZojQ580",
    "outputId": "128eeff3-30b4-4487-f77e-1d64b72b81fb"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize and train the Support Vector Machine model\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "svm_y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the SVM model\n",
    "svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, svm_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gkrk2TdkRCbE",
    "outputId": "fb959810-3d2a-415b-d0fc-9f00dd926b5b"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "xgb_y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the XGBoost model\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, xgb_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-deP1cNSAsE",
    "outputId": "1215b0bc-4f04-4f2a-b15c-e14c4803dc67"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Assuming the baseline model was Logistic Regression and it performed well, we use it as the final model\n",
    "final_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the final model on the training data\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the final model on the test data\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "final_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Classification report\n",
    "classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "final_accuracy, classification_rep, conf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAMtah3qTv0V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmZ1v-v1IIyW"
   },
   "source": [
    "# **Evaluation**\n",
    "\n",
    "## 4. Final Model Selection & Improvement Analysis\n",
    "\n",
    "### Final Model:\n",
    "By comparing the results of each model; Logistic Regression, Random Forest, SVM, and XGBoost, we can giew the progression in terms of accuracy and F1-score. Using these comparisons, the model with the best performance is selected for final evaluation and interpretation.\n",
    "\n",
    "### Selection\n",
    "After comparing model performance, model chosen is **Logistic Regression** as the final model. This decision is based on:\n",
    "\n",
    "- **Accuracy**: Logistic Regression achieved an accuracy of **0.7391**, the highest between all tested.\n",
    "- **Class Imbalance Handling**:  Logistic Regression struggles with class 3 but performed well on the majority of classes, especially on class 1 (high recall of 94%).\n",
    "- **Model Simplicity**: Logistic Regression is a relatively simple model, making it easier to interpret and deploy in real-world applications.\n",
    "\n",
    "Looking at performance across all classes, it is justified that Logistic regression was the best.\n",
    "\n",
    "## Evaluation Metrics\n",
    "We used these metrics for Logistic Regression model:\n",
    "\n",
    "- **Accuracy**: Provides a general overview of model performance but may not be sufficient due to class imbalance.\n",
    "- **Precision and Recall**: Helps evaluate how model distinguishes between classes.\n",
    "- **F1-Score**: Balances precision and recall to give a better measure of the model effective ability.\n",
    "- **Confusion Matrix**: Identifies misclassification patterns, which can be useful for refining the model.\n",
    "\n",
    "### Final Evaluation Results\n",
    "\n",
    "- **Accuracy**: 0.7391 = 73.91%\n",
    "- **Precision and Recall**:\n",
    "  - **Class 0**: Precision = 79%, Recall = 66%\n",
    "  - **Class 1**: Precision = 71%, Recall = 94%\n",
    "  - **Class 2**: Precision = 87%, Recall = 20%\n",
    "  - **Class 3**: Precision = 0%, Recall = 0%\n",
    "- **F1-Score**:\n",
    "  - **Highest for Class 1 (0.81)**\n",
    "  - **Lowest for Class 3 (0.00)**\n",
    "\n",
    "### Interpreting the Evaluation Results\n",
    "- The model  predicts **73.91% of test samples**.\n",
    "- It performs well on **majority classes ** but hard time with **rare accident causes (Class 2 & Class 3)**.\n",
    "- **Class imbalance remains an issue**, as **Class 3 is not predicted at all**.\n",
    "\n",
    "## Next Steps: Refinement\n",
    "\n",
    "### 1. Handling Class Imbalance\n",
    "- **Oversampling Minority Classes**: Implement **SMOTE (Synthetic Minority Over-sampling Technique)** to generate synthetic examples for underrepresented classes.\n",
    "- **Class Weight Adjustment**: Modify the class weights during training to ensure the model focuses on the minority classes.\n",
    "\n",
    "### 2. Model Improvement Techniques\n",
    "- **Hyperparameter Tuning**: Use **Grid Search** to find the optimal parameters for Logistic Regression and improve generalization.\n",
    "- **Ensemble Methods**: Explore **Random Forests or XGBoost** to combine multiple models for better prediction accuracy.\n",
    "\n",
    "### 3. Addressing Rare Classes in the Data\n",
    "- **Improving Data Collection**: Gather additional data for underrepresented classes to enhance model training.\n",
    "\n",
    "### 4. Model Monitoring & Deployment\n",
    "- **Model Monitoring**: Once deployed, track performance metrics over time.\n",
    "- **Retraining**: Update the model with new data periodically to maintain accuracy and adaptability.\n",
    "\n",
    "By implementing these, we can improve the model ability to predict minority classes and enhance its real-world application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvoMhfdJIJW9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
